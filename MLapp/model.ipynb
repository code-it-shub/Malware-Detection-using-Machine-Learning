{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.ensemble as sk\n",
    "import sklearn.ensemble as ske \n",
    "from sklearn import tree \n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of file in the dataset is  138047\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('data.csv',sep='|')\n",
    "x=data.drop(['Name','legitimate','md5'],axis=1)\n",
    "y = data['legitimate'].values\n",
    "print(\" Total number of file in the dataset is \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ML project\\Malware_detection\\env\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For best feature selection We are using ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "selected_feature = model.fit(x,y)\n",
    "modal_feature = SelectFromModel(selected_feature,prefit=True)\n",
    "X_new = modal_feature.transform(x)\n",
    "nb_features = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 features identified as important:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.2)\n",
    "\n",
    "features = []\n",
    "\n",
    "print('%i features identified as important:' % nb_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Testing we have taken  27610 files\n"
     ]
    }
   ],
   "source": [
    "print(\"For Testing we have taken \",len(X_test) ,\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature DllCharacteristics (0.168361)\n",
      "2. feature Machine (0.113157)\n",
      "3. feature Characteristics (0.088354)\n",
      "4. feature VersionInformationSize (0.074158)\n",
      "5. feature Subsystem (0.056845)\n",
      "6. feature SectionsMaxEntropy (0.050371)\n",
      "7. feature MajorSubsystemVersion (0.048051)\n",
      "8. feature ImageBase (0.046433)\n",
      "9. feature SizeOfOptionalHeader (0.045487)\n",
      "10. feature ResourcesMaxEntropy (0.044354)\n",
      "11. feature SectionsMinEntropy (0.025901)\n",
      "12. feature ResourcesMinEntropy (0.024866)\n",
      "13. feature MajorOperatingSystemVersion (0.020662)\n",
      "14. feature SizeOfStackReserve (0.020024)\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(model.feature_importances_)[::-1][:nb_features]\n",
    "for f in range(nb_features):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, data.columns[2+indices[f]], model.feature_importances_[indices[f]]))\n",
    "\n",
    "# mean adding to the empty 'features' array the 'important features'\n",
    "for f in sorted(np.argsort(model.feature_importances_)[::-1][:nb_features]):#[::-1] mean start with last towards first \n",
    "    features.append(data.columns[2+f])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now testing algorithms\n",
      "DecisionTree : 99.152481 %\n",
      "RandomForest : 99.406012 %\n",
      "GradientBoosting : 98.804781 %\n",
      "AdaBoost : 98.649040 %\n",
      "GNB : 70.079681 %\n",
      "results\n",
      "\n",
      "Winner algorithm is RandomForest with a 99.406012 % success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Algorithm comparison\n",
    "algorithms = {\n",
    "        \"DecisionTree\": tree.DecisionTreeClassifier(max_depth=12),\n",
    "        #The max_depth parameter denotes maximum depth of the tree.\n",
    "\n",
    "        \"RandomForest\": ske.RandomForestClassifier(n_estimators=50),\n",
    "         #n_estimators ==The number of trees in the forest.\n",
    "    \n",
    "        \"GradientBoosting\": ske.GradientBoostingClassifier(n_estimators=50),\n",
    "        \"AdaBoost\": ske.AdaBoostClassifier(n_estimators=100),\n",
    "         #Ada mean Adaptive\n",
    "        \"GNB\": GaussianNB()\n",
    "        #Bayes theorem is based on conditional probability. The conditional probability helps us calculating the probability  something will happen\n",
    "\n",
    "\t}\n",
    "results = {}\n",
    "print(\"\\nNow testing algorithms\")\n",
    "for algo in algorithms:\n",
    "    clf = algorithms[algo]\n",
    "    clf.fit(X_train, y_train)#fit may be called as 'trained'\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"%s : %f %%\" % (algo, score*100))\n",
    "    results[algo] = score\n",
    "    \n",
    "\n",
    "\n",
    "print('results')\n",
    "winner = max(results, key=results.get)\n",
    "print('\\nWinner algorithm is %s with a %f %% success' % (winner, results[winner]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving algorithm and feature list in classifier directory...\n",
      "Saved\n"
     ]
    }
   ],
   "source": [
    "print('Saving algorithm and feature list in classifier directory...')\n",
    "# Save the winning algorithm to a pickle file\n",
    "with open('classifier/classifier.pkl', 'wb') as f:\n",
    "    joblib.dump(algorithms[winner], f)\n",
    "\n",
    "# Save the features to a pickle file\n",
    "with open('classifier/features.pkl', 'wb') as f:\n",
    "    pickle.dump(features, f)\n",
    "\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
